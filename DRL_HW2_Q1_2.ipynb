{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMAPYGqBdZEn"
      },
      "source": [
        "# **Deep Reinforcement Learning Class ‚Äì Spring 2025 Assignment 2**\n",
        "\n",
        "In this assignment, you will explore multiple exploration and exploitation methods, as well as variants of Monte Carlo Tree Search (MCTS), Temporal Difference (TD) learning, n-tuple approximation, and other key reinforcement learning techniques.  \n",
        "\n",
        "You will need to <font color='blue'>answer the bolded questions</font> and <font color='blue'>fill in the missing code snippets (marked by **TODO**)</font>.\n",
        "\n",
        "Make a copy of this notebook using **File > Save a copy in Drive** and edit it with your answers.  \n",
        "\n",
        "**WARNING:** Do not include your name or any other personal identification information in this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTA94i9TdnXV"
      },
      "source": [
        "### **Setup**\n",
        "\n",
        "Run the following code to set up the necessary imports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1GbXdsmPMox",
        "outputId": "9b0916d4-a1fe-4229-d8bc-49cc9744bcf3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (2.3 kB)\n",
            "Downloading numpy-1.23.5-cp38-cp38-macosx_10_9_x86_64.whl (18.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.3\n",
            "    Uninstalling numpy-1.24.3:\n",
            "      Successfully uninstalled numpy-1.24.3\n",
            "Successfully installed numpy-1.23.5\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.8 install --upgrade pip\u001b[0m\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gym in /Users/liyijing/Library/Python/3.8/lib/python/site-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /Users/liyijing/Library/Python/3.8/lib/python/site-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/liyijing/Library/Python/3.8/lib/python/site-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /Users/liyijing/Library/Python/3.8/lib/python/site-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gym) (6.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.8 install --upgrade pip\u001b[0m\n",
            "Cloning into 'gym-bandits'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Total 73 (delta 0), reused 0 (delta 0), pack-reused 73 (from 1)\u001b[K\n",
            "Receiving objects: 100% (73/73), 14.67 KiB | 2.93 MiB/s, done.\n",
            "Resolving deltas: 100% (32/32), done.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.8 install --upgrade pip\u001b[0m\n",
            "\u001b[31mERROR: Invalid requirement: '/content/gym-bandits/.': Expected package name at the start of dependency specifier\n",
            "    /content/gym-bandits/.\n",
            "    ^\n",
            "Hint: It looks like a path. File '/content/gym-bandits/.' does not exist.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install gym\n",
        "!git clone https://github.com/JKCooper2/gym-bandits.git\n",
        "!pip install /content/gym-bandits/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EElcHk4kHXGE",
        "outputId": "4807677e-f321-4163-b3b9-76e469f6b37f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import gym\n",
        "import gym_bandits\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create Bandit environment\n",
        "env = gym.make('BanditTenArmedGaussian-v0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chI7t8bRYZ7r"
      },
      "source": [
        "# **Question 1: Exploring and Enhancing MCTS with Exploration Strategies and TD Learning**  \n",
        "\n",
        "In this question, you will explore different **exploration strategies**, implement a **basic Monte Carlo Tree Search (MCTS)** with UCT, and enhance it using **Temporal Difference (TD) learning** with **n-tuple approximation** in the **2048 environment**.  \n",
        "\n",
        "You will start by comparing multiple **exploration-exploitation** techniques in a **multi-armed bandit** environment, then move on to **MCTS with UCT**, and finally integrate **TD-learning** to improve MCTS decision-making.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Steps:**  \n",
        "\n",
        "### **1. Compare Exploration Strategies in Multi-Armed Bandits**  \n",
        "- Implement and analyze **Exploration-First, Epsilon-Greedy, and UCB1** in a bandit setting.  \n",
        "- Compare how these methods balance **exploration and exploitation** and evaluate their performance over multiple runs.  \n",
        "- Discuss which method is most effective based on **average reward trends** and **long-term convergence behavior**.  \n",
        "\n",
        "### **2. Implement Basic MCTS with UCT for 2048**  \n",
        "- Develop a **Monte Carlo Tree Search (MCTS)** algorithm using **Upper Confidence Bound for Trees (UCT)**.  \n",
        "- Implement the **Selection, Expansion, Simulation, and Backpropagation** phases.  \n",
        "- Use **random rollouts** to estimate state values and guide decision-making.  \n",
        "\n",
        "### **3. Enhance MCTS with TD Learning and N-Tuple Approximation**  \n",
        "- Implement **Temporal Difference (TD) learning** with **n-tuple approximation** to estimate state values.  \n",
        "- Integrate this learned value function into **MCTS**, replacing the random rollout with a **TD-based evaluation**.  \n",
        "- Observe how the **learned value function improves MCTS efficiency and decision quality**.  \n",
        "\n",
        "By the end of this question, you will have built a **stronger MCTS agent** that intelligently balances exploration and exploitation and leverages **value function learning** to improve search accuracy.  \n",
        "\n",
        "\n",
        "üéØ How to Earn 15 Points?\n",
        "\n",
        "1Ô∏è‚É£ Correct Implementation of Step 1 Agents (2 points)\n",
        "/ Discussion of Three Methods (3 points)\n",
        "\n",
        "2Ô∏è‚É£ Implementation of MCTS with UCT Formula and Rollout (4 points)\n",
        "**Note: Points may be deducted for errors in formulas or function implementation.**\n",
        "\n",
        "3Ô∏è‚É£ Correct Implementation of TD Learning + Explanation (4 points)\n",
        "\n",
        "4Ô∏è‚É£ Integration of Value Approximator into MCTS + Explanation (2 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqOZfqOOIBkQ"
      },
      "source": [
        "## Explore First Agent\n",
        "\n",
        "In this cell we will dicuss an algorithm to solve bandits where,\n",
        "- Exploration Phase: For the first N (defined as `max_explore` in the code) steps the agent takes random actions to estimate the value of different arms.\n",
        "- Exploitation Phase: In each step after that, the agent identifies the best arm based on the information it aggregated so far.\n",
        "Notice that the agent keeps updating its prediction even after the inital N steps.\n",
        "\n",
        "We will now implement this agent below.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_5u-mOUkoYq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ExplorationFirstAgent:\n",
        "    def __init__(self, k=10, exploration_steps=100):\n",
        "        \"\"\"\n",
        "        Initialize an Exploration-First Agent.\n",
        "\n",
        "        Args:\n",
        "        k (int): Number of possible actions.\n",
        "        exploration_steps (int): Number of initial steps dedicated to pure exploration.\n",
        "\n",
        "        Attributes:\n",
        "        Q (np.array): Estimated value for each action.\n",
        "        N (np.array): Count of times each action has been selected.\n",
        "        t (int): Current time step.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.exploration_steps = exploration_steps\n",
        "        self.Q = np.zeros(k)\n",
        "        self.N = np.zeros(k)\n",
        "        self.t = 0\n",
        "\n",
        "    def select_action(self):\n",
        "        # For the first N steps, perform exploration; afterward, exploit\n",
        "        if self.t < self.exploration_steps:\n",
        "            action = np.random.randint(self.k)\n",
        "        else:\n",
        "            action = np.argmax(self.Q)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # Update N, Q, and t\n",
        "        self.N[action] += 1\n",
        "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n",
        "        self.t += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSUi5lhXIUP6"
      },
      "source": [
        "## Epsilon-greedy Agent\n",
        "\n",
        "A popular method of simultaneoulsy exploring/exploiting is $\\epsilon$-greedy exploration. The main idea is to:\n",
        "- Sample the (estimated) best action with probability $1-\\epsilon$\n",
        "- Perform a random action with probability $\\epsilon$\n",
        "\n",
        "By changing $\\epsilon$, we can control if the agent is conservative or exploratory. We will now implement this agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpSOn6AKIs03"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class EpsilonGreedyAgent:\n",
        "    def __init__(self, k=10, epsilon=0.1):\n",
        "        \"\"\"\n",
        "        Initialize an Epsilon-Greedy Agent.\n",
        "\n",
        "        Args:\n",
        "        epsilon (float): Probability of selecting a random action (exploration).\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.epsilon = epsilon\n",
        "        self.Q = np.zeros(k)\n",
        "        self.N = np.zeros(k)\n",
        "\n",
        "    def select_action(self):\n",
        "        # With probability epsilon, select a random action (exploration); otherwise, select the action with the highest Q-value (exploitation)\n",
        "        if np.random.random() < self.epsilon:\n",
        "            action = np.random.randint(self.k)\n",
        "        else:\n",
        "            action = np.argmax(self.Q)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # Update N, Q\n",
        "        self.N[action] += 1\n",
        "        self.Q[action] += (reward - self.Q[action]) / self.N[action]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZgr4hVCIt53"
      },
      "source": [
        "## UCB Agent\n",
        "\n",
        "Unlike **Exploration-First**, which separates exploration and exploitation into distinct phases, or **Epsilon-Greedy**, which relies on random exploration, the **UCB (Upper Confidence Bound) agent** dynamically balances exploration and exploitation based on uncertainty.  \n",
        "\n",
        "The agent should be able to **intelligently decide when to explore and when to exploit**, rather than following a predefined exploration schedule or relying on random action selection.  \n",
        "\n",
        "Your task is to implement the `update_Q` and `get_action` methods for a UCB agent, ensuring that it effectively leverages its confidence bounds to make optimal decisions.  \n",
        "\n",
        "---\n",
        "\n",
        "## UCB1 Formula\n",
        "\n",
        "The UCB1 algorithm selects an action `a` based on the following equation:\n",
        "\n",
        "$Q(a) + c \\cdot \\sqrt{\\frac{\\log t}{N(a)}}$\n",
        "\n",
        "where:\n",
        "- `Q(a)`: Estimated reward for action `a` (average of observed rewards).  \n",
        "- `N(a)`: Number of times action `a` has been selected.  \n",
        "- `t`: Total number of trials (time step).  \n",
        "- `c`: Exploration parameter (typically set to 1.0 or tuned based on problem complexity).  \n",
        "\n",
        "To implement this, you need to:\n",
        "1. **Ensure that each action is selected at least once** before applying the UCB formula.  \n",
        "2. **Use the equation to compute UCB values** for all actions and select the one with the highest value.  \n",
        "3. **Update the estimated rewards `Q(a)` after each selection** using incremental averaging:  \n",
        "\n",
        "$Q(a) \\gets Q(a) + \\frac{(R - Q(a))}{N(a)}$\n",
        "\n",
        "where `R` is the reward received from selecting action `a`.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQlds7c_JFEg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class UCB1Agent:\n",
        "    def __init__(self, k=10, c=1.0):\n",
        "        \"\"\"\n",
        "        Initialize a UCB1 Agent.\n",
        "\n",
        "        Args:\n",
        "        c (float): Exploration parameter that controls the balance between exploration and exploitation.\n",
        "        \"\"\"\n",
        "        self.k = k\n",
        "        self.c = c\n",
        "        self.Q = np.zeros(k)\n",
        "        self.N = np.zeros(k)\n",
        "        self.t = 0\n",
        "\n",
        "    def select_action(self):\n",
        "        # Implement UCB1 action selection\n",
        "        # If any action hasn't been selected yet, select it first.\n",
        "        for action in range(self.k):\n",
        "            if self.N[action] == 0:\n",
        "                return action\n",
        "\n",
        "        # Calculate UCB values for each action.\n",
        "        # Note: self.t is at least k at this point, so log(self.t) is defined.\n",
        "        ucb_values = self.Q + self.c * np.sqrt(np.log(self.t) / self.N)\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "\n",
        "    def update(self, action, reward):\n",
        "        # Update N, Q, t\n",
        "        self.t += 1\n",
        "        self.N[action] += 1\n",
        "        # Incremental update of Q value using averaging\n",
        "        self.Q[action] = self.Q[action] + (reward - self.Q[action]) / self.N[action]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqFxqhH4OU_k"
      },
      "source": [
        "## Simulation and Comparison of Exploration Strategies\n",
        "\n",
        "This experiment compares the performance of three exploration strategies: **Exploration-First, Epsilon-Greedy, and UCB1**, in a **multi-armed bandit environment**. The simulation runs each strategy for **1000** steps across **50** independent runs to evaluate their average performance.\n",
        "\n",
        "You should not modify this code‚Äîjust run it to obtain the performance comparison between the three agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "l_PyLxUWJS8o",
        "outputId": "8b8724a2-b705-4caf-9ee0-836362a1c7db"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final 200-step Avg Reward - Exploration-First: 1.979, Epsilon-Greedy: 1.807, UCB1: 1.988\n",
            "‚úÖ UCB1 performs the best in the long run!\n"
          ]
        }
      ],
      "source": [
        "def simulate(env, agent, steps=1000):\n",
        "    rewards = np.zeros(steps)\n",
        "\n",
        "    for t in range(steps):\n",
        "        action = agent.select_action()\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        agent.update(action, reward)\n",
        "        rewards[t] = reward\n",
        "\n",
        "    return rewards\n",
        "\n",
        "num_runs = 50\n",
        "steps = 1000\n",
        "\n",
        "all_rewards_exp_first = []\n",
        "all_rewards_eg = []\n",
        "all_rewards_ucb = []\n",
        "\n",
        "for _ in range(num_runs):\n",
        "    env.reset()\n",
        "    exp_first_agent = ExplorationFirstAgent(k=10, exploration_steps=100)\n",
        "    rewards_exp_first = simulate(env, exp_first_agent, steps)\n",
        "    all_rewards_exp_first.append(rewards_exp_first)\n",
        "\n",
        "    env.reset()\n",
        "    epsilon_greedy_agent = EpsilonGreedyAgent(k=10, epsilon=0.1)\n",
        "    rewards_eg = simulate(env, epsilon_greedy_agent, steps)\n",
        "    all_rewards_eg.append(rewards_eg)\n",
        "\n",
        "    env.reset()\n",
        "    ucb_agent = UCB1Agent(k=10, c=1.0)\n",
        "    rewards_ucb = simulate(env, ucb_agent, steps)\n",
        "    all_rewards_ucb.append(rewards_ucb)\n",
        "\n",
        "avg_rewards_exp_first = np.mean(all_rewards_exp_first, axis=0)\n",
        "avg_rewards_eg = np.mean(all_rewards_eg, axis=0)\n",
        "avg_rewards_ucb = np.mean(all_rewards_ucb, axis=0)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(avg_rewards_exp_first, label=\"Exploration-First\", color=\"green\")\n",
        "plt.plot(avg_rewards_eg, label=\"Epsilon-Greedy\", color=\"blue\")\n",
        "plt.plot(avg_rewards_ucb, label=\"UCB1\", color=\"orange\")\n",
        "\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Comparison of Exploration-First, Epsilon-Greedy, and UCB1 (Averaged Over 50 Runs)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "final_avg_exp_first = np.mean(avg_rewards_exp_first[-200:])\n",
        "final_avg_eg = np.mean(avg_rewards_eg[-200:])\n",
        "final_avg_ucb = np.mean(avg_rewards_ucb[-200:])\n",
        "\n",
        "print(f\"Final 200-step Avg Reward - Exploration-First: {final_avg_exp_first:.3f}, Epsilon-Greedy: {final_avg_eg:.3f}, UCB1: {final_avg_ucb:.3f}\")\n",
        "\n",
        "if final_avg_exp_first > final_avg_eg and final_avg_exp_first > final_avg_ucb:\n",
        "    print(\"‚úÖ Exploration-First performs the best in the long run!\")\n",
        "elif final_avg_eg > final_avg_exp_first and final_avg_eg > final_avg_ucb:\n",
        "    print(\"‚úÖ Epsilon-Greedy performs the best in the long run!\")\n",
        "else:\n",
        "    print(\"‚úÖ UCB1 performs the best in the long run!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsrh3mUETPcC"
      },
      "source": [
        "**Question (3pts):**  \n",
        "\n",
        "Based on the plotted results, which exploration strategy (**Exploration-First, Epsilon-Greedy, or UCB1**) performed the best? Why?  \n",
        "\n",
        "Analyze the reasons behind this outcome. How do different strategies balance exploration and exploitation, leading to these results?  \n",
        "Provide an explanation based on the observed trends, identifying which strategy was the most effective in this environment and comparing its advantages and limitations to the others.  \n",
        "\n",
        "**Answer:**\n",
        "UCB1 outperforms the other strategies by using an informed, dynamic approach that adjusts exploration based on uncertainty. Unlike Exploration-First and Epsilon-Greedy, UCB1 systematically favors arms with high uncertainty through its confidence-bound term, ensuring that each action is explored sufficiently without wasting resources on arms that are already known to be suboptimal.\n",
        "\n",
        "This efficient balance allows UCB1 to converge faster and maintain higher long-term rewards in stationary environments, as it continuously adapts its action selection based on observed performance. Overall, the targeted exploration inherent in UCB1 gives it a significant edge over the more rigid or random exploration methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYrvdcjPsKrW"
      },
      "source": [
        "## Applying Exploration Strategies to 2048: MCTS and TD-Learning\n",
        "\n",
        "Now that you have explored different **exploration strategies** and gained an understanding of how they balance exploration and exploitation, it‚Äôs time to apply these concepts to a more complex decision-making problem: **the game of 2048**.  \n",
        "\n",
        "In this section, you will implement **Monte Carlo Tree Search (MCTS) with UCT** and enhance it using **Temporal Difference (TD) learning with n-tuple approximation**. Before diving into the implementation, take a moment to develop an intuition for the game itself. You can try playing 2048 at this [link](https://play2048.co/) to familiarize yourself with its mechanics.  \n",
        "\n",
        "You **do not** need to modify the provided 2048 environment code. However, if you find that certain adjustments could improve your training process, feel free to make changes. Just ensure that your modifications **do not alter the fundamental behavior of the environment**.  \n",
        "\n",
        "***Note: While some versions of 2048 terminate as soon as the 2048 tile is reached, our environment will continue running until no legal moves remain.***  \n",
        "\n",
        "Now, let‚Äôs move forward with implementing **MCTS and TD-learning**, leveraging what we've learned about exploration and structured search to improve decision-making in 2048!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HhYF13BzawAm"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/liyijing/Library/Python/3.8/lib/python/site-packages/gym/envs/registration.py:307: DeprecationWarning: The package name gym_minigrid has been deprecated in favor of minigrid. Please uninstall gym_minigrid and install minigrid with `pip install minigrid`. Future releases will be maintained under the new package name minigrid.\n",
            "  fn()\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "from gym import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "COLOR_MAP = {\n",
        "    0: \"#cdc1b4\", 2: \"#eee4da\", 4: \"#ede0c8\", 8: \"#f2b179\",\n",
        "    16: \"#f59563\", 32: \"#f67c5f\", 64: \"#f65e3b\", 128: \"#edcf72\",\n",
        "    256: \"#edcc61\", 512: \"#edc850\", 1024: \"#edc53f\", 2048: \"#edc22e\",\n",
        "    4096: \"#3c3a32\", 8192: \"#3c3a32\", 16384: \"#3c3a32\", 32768: \"#3c3a32\"\n",
        "}\n",
        "TEXT_COLOR = {\n",
        "    2: \"#776e65\", 4: \"#776e65\", 8: \"#f9f6f2\", 16: \"#f9f6f2\",\n",
        "    32: \"#f9f6f2\", 64: \"#f9f6f2\", 128: \"#f9f6f2\", 256: \"#f9f6f2\",\n",
        "    512: \"#f9f6f2\", 1024: \"#f9f6f2\", 2048: \"#f9f6f2\", 4096: \"#f9f6f2\"\n",
        "}\n",
        "\n",
        "class Game2048Env(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(Game2048Env, self).__init__()\n",
        "\n",
        "        self.size = 4\n",
        "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
        "        self.score = 0\n",
        "\n",
        "        # Action space: 0: up, 1: down, 2: left, 3: right\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
        "\n",
        "        self.last_move_valid = True\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.board = np.zeros((self.size, self.size), dtype=int)\n",
        "        self.score = 0\n",
        "        self.add_random_tile()\n",
        "        self.add_random_tile()\n",
        "        return self.board\n",
        "\n",
        "    def add_random_tile(self):\n",
        "        empty_cells = list(zip(*np.where(self.board == 0)))\n",
        "        if empty_cells:\n",
        "            x, y = random.choice(empty_cells)\n",
        "            self.board[x, y] = 2 if random.random() < 0.9 else 4\n",
        "\n",
        "    def compress(self, row):\n",
        "        new_row = row[row != 0]\n",
        "        new_row = np.pad(new_row, (0, self.size - len(new_row)), mode='constant')\n",
        "        return new_row\n",
        "\n",
        "    def merge(self, row):\n",
        "        for i in range(len(row) - 1):\n",
        "            if row[i] == row[i + 1] and row[i] != 0:\n",
        "                row[i] *= 2\n",
        "                row[i + 1] = 0\n",
        "                self.score += row[i]\n",
        "        return row\n",
        "\n",
        "    def move_left(self):\n",
        "        moved = False\n",
        "        for i in range(self.size):\n",
        "            original_row = self.board[i].copy()\n",
        "            new_row = self.compress(self.board[i])\n",
        "            new_row = self.merge(new_row)\n",
        "            new_row = self.compress(new_row)\n",
        "            self.board[i] = new_row\n",
        "            if not np.array_equal(original_row, self.board[i]):\n",
        "                moved = True\n",
        "        return moved\n",
        "\n",
        "    def move_right(self):\n",
        "        moved = False\n",
        "        for i in range(self.size):\n",
        "            original_row = self.board[i].copy()\n",
        "            reversed_row = self.board[i][::-1]\n",
        "            reversed_row = self.compress(reversed_row)\n",
        "            reversed_row = self.merge(reversed_row)\n",
        "            reversed_row = self.compress(reversed_row)\n",
        "            self.board[i] = reversed_row[::-1]\n",
        "            if not np.array_equal(original_row, self.board[i]):\n",
        "                moved = True\n",
        "        return moved\n",
        "\n",
        "    def move_up(self):\n",
        "        moved = False\n",
        "        for j in range(self.size):\n",
        "            original_col = self.board[:, j].copy()\n",
        "            col = self.compress(self.board[:, j])\n",
        "            col = self.merge(col)\n",
        "            col = self.compress(col)\n",
        "            self.board[:, j] = col\n",
        "            if not np.array_equal(original_col, self.board[:, j]):\n",
        "                moved = True\n",
        "        return moved\n",
        "\n",
        "    def move_down(self):\n",
        "        moved = False\n",
        "        for j in range(self.size):\n",
        "            original_col = self.board[:, j].copy()\n",
        "            reversed_col = self.board[:, j][::-1]\n",
        "            reversed_col = self.compress(reversed_col)\n",
        "            reversed_col = self.merge(reversed_col)\n",
        "            reversed_col = self.compress(reversed_col)\n",
        "            self.board[:, j] = reversed_col[::-1]\n",
        "            if not np.array_equal(original_col, self.board[:, j]):\n",
        "                moved = True\n",
        "        return moved\n",
        "\n",
        "    def is_game_over(self):\n",
        "        if np.any(self.board == 0):\n",
        "            return False\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size - 1):\n",
        "                if self.board[i, j] == self.board[i, j+1]:\n",
        "                    return False\n",
        "        for j in range(self.size):\n",
        "            for i in range(self.size - 1):\n",
        "                if self.board[i, j] == self.board[i+1, j]:\n",
        "                    return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action), \"Invalid action\"\n",
        "\n",
        "        if action == 0:\n",
        "            moved = self.move_up()\n",
        "        elif action == 1:\n",
        "            moved = self.move_down()\n",
        "        elif action == 2:\n",
        "            moved = self.move_left()\n",
        "        elif action == 3:\n",
        "            moved = self.move_right()\n",
        "        else:\n",
        "            moved = False\n",
        "\n",
        "        self.last_move_valid = moved\n",
        "\n",
        "        if moved:\n",
        "            self.add_random_tile()\n",
        "\n",
        "        done = self.is_game_over()\n",
        "\n",
        "        return self.board, self.score, done, {}\n",
        "\n",
        "    def render(self, mode=\"human\", action=None):\n",
        "        fig, ax = plt.subplots(figsize=(4, 4))\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        ax.set_xlim(-0.5, self.size - 0.5)\n",
        "        ax.set_ylim(-0.5, self.size - 0.5)\n",
        "\n",
        "        for i in range(self.size):\n",
        "            for j in range(self.size):\n",
        "                value = self.board[i, j]\n",
        "                color = COLOR_MAP.get(value, \"#3c3a32\")\n",
        "                text_color = TEXT_COLOR.get(value, \"white\")\n",
        "                rect = plt.Rectangle((j - 0.5, i - 0.5), 1, 1, facecolor=color, edgecolor=\"black\")\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "                if value != 0:\n",
        "                    ax.text(j, i, str(value), ha='center', va='center',\n",
        "                            fontsize=16, fontweight='bold', color=text_color)\n",
        "        title = f\"score: {self.score}\"\n",
        "        if action is not None:\n",
        "            title += f\" | action: {self.actions[action]}\"\n",
        "        plt.title(title)\n",
        "        plt.gca().invert_yaxis()\n",
        "        plt.show()\n",
        "\n",
        "    def simulate_row_move(self, row):\n",
        "        new_row = row[row != 0]\n",
        "        new_row = np.pad(new_row, (0, self.size - len(new_row)), mode='constant')\n",
        "        for i in range(len(new_row) - 1):\n",
        "            if new_row[i] == new_row[i + 1] and new_row[i] != 0:\n",
        "                new_row[i] *= 2\n",
        "                new_row[i + 1] = 0\n",
        "        new_row = new_row[new_row != 0]\n",
        "        new_row = np.pad(new_row, (0, self.size - len(new_row)), mode='constant')\n",
        "        return new_row\n",
        "\n",
        "    def is_move_legal(self, action):\n",
        "        temp_board = self.board.copy()\n",
        "\n",
        "        if action == 0:  # Move up\n",
        "            for j in range(self.size):\n",
        "                col = temp_board[:, j]\n",
        "                new_col = self.simulate_row_move(col)\n",
        "                temp_board[:, j] = new_col\n",
        "        elif action == 1:  # Move down\n",
        "            for j in range(self.size):\n",
        "                col = temp_board[:, j][::-1]\n",
        "                new_col = self.simulate_row_move(col)\n",
        "                temp_board[:, j] = new_col[::-1]\n",
        "        elif action == 2:  # Move left\n",
        "            for i in range(self.size):\n",
        "                row = temp_board[i]\n",
        "                temp_board[i] = self.simulate_row_move(row)\n",
        "        elif action == 3:  # Move right\n",
        "            for i in range(self.size):\n",
        "                row = temp_board[i][::-1]\n",
        "                new_row = self.simulate_row_move(row)\n",
        "                temp_board[i] = new_row[::-1]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "        return not np.array_equal(self.board, temp_board)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MNbRLLv1s6T"
      },
      "source": [
        "This snippet initializes and runs a random agent in the 2048 environment, selecting moves randomly from the set of legal actions.\n",
        "\n",
        "You can execute this code to observe the gameplay and get an estimate of the score when playing randomly. This will serve as a baseline to compare against more advanced techniques such as MCTS and TD-learning, helping you evaluate their effectiveness in improving performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 24167
        },
        "collapsed": true,
        "id": "YXbs4nVRe25y",
        "outputId": "09bc48b5-53eb-4584-f28a-94a98a43124c"
      },
      "outputs": [],
      "source": [
        "env = Game2048Env()\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    legal_moves = [action for action in [0, 1, 2, 3] if env.is_move_legal(action)]\n",
        "    action = random.choice(legal_moves)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render(action=action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ6oTIYZ2r_K"
      },
      "source": [
        "## Introduction to MCTS with UCT and Rollout\n",
        "In this section, we will implement Monte Carlo Tree Search (MCTS) using only the Upper Confidence Bound for Trees (UCT) formula and rollout, without any policy or value approximation.\n",
        "\n",
        "MCTS consists of four main steps:\n",
        "1. Selection ‚Äì Traverse the tree by selecting child nodes based on the UCT formula, balancing exploration and exploitation.\n",
        "2. Expansion ‚Äì If a node has untried actions, expand the tree by adding a new child node.\n",
        "3. Rollout (Simulation) ‚Äì Perform a random simulation from the newly expanded node to estimate its value.\n",
        "4. Backpropagation ‚Äì Update the node's statistics by propagating the simulation result back up the tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UtIoDEG1gUzL"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# UCT Node for MCTS\n",
        "class UCTNode:\n",
        "    def __init__(self, state, score, parent=None, action=None):\n",
        "        \"\"\"\n",
        "        state: current board state (numpy array)\n",
        "        score: cumulative score at this node\n",
        "        parent: parent node (None for root)\n",
        "        action: action taken from parent to reach this node\n",
        "        \"\"\"\n",
        "        self.state = state\n",
        "        self.score = score\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.total_reward = 0.0\n",
        "        self.untried_actions = [a for a in range(4) if env.is_move_legal(a)]\n",
        "\n",
        "    def fully_expanded(self):\n",
        "\t\t# A node is fully expanded if no legal actions remain untried.\n",
        "        return len(self.untried_actions) == 0\n",
        "\n",
        "\n",
        "class UCTMCTS:\n",
        "    def __init__(self, env, iterations=500, exploration_constant=1.41, rollout_depth=10):\n",
        "        self.env = env\n",
        "        self.iterations = iterations\n",
        "        self.c = exploration_constant  # Balances exploration and exploitation\n",
        "        self.rollout_depth = rollout_depth\n",
        "\n",
        "    def create_env_from_state(self, state, score):\n",
        "        \"\"\"\n",
        "        Creates a deep copy of the environment with a given board state and score.\n",
        "        \"\"\"\n",
        "        new_env = copy.deepcopy(self.env)\n",
        "        new_env.board = state.copy()\n",
        "        new_env.score = score\n",
        "        return new_env\n",
        "\n",
        "    def select_child(self, node):\n",
        "        # Use the UCT formula: Q + c * sqrt(log(parent_visits)/child_visits) to select the child\n",
        "        best_value = -float('inf')\n",
        "        selected_child = None\n",
        "        for child in node.children.values():\n",
        "            uct_value = (child.total_reward / child.visits) + self.c * math.sqrt(math.log(node.visits) / child.visits)\n",
        "            if uct_value > best_value:\n",
        "                best_value = uct_value\n",
        "                selected_child = child\n",
        "        return selected_child\n",
        "\n",
        "    def rollout(self, sim_env, depth):\n",
        "        # Perform a random rollout from the current state up to the specified depth.\n",
        "        initial_score = sim_env.score\n",
        "        current_depth = 0\n",
        "        while current_depth < depth and not sim_env.is_game_over():\n",
        "            legal_actions = [a for a in range(4) if sim_env.is_move_legal(a)]\n",
        "            if not legal_actions:\n",
        "                break\n",
        "            action = random.choice(legal_actions)\n",
        "            sim_env.step(action)\n",
        "            current_depth += 1\n",
        "        # Return the incremental reward gained during the rollout.\n",
        "        return sim_env.score - initial_score\n",
        "\n",
        "    def backpropagate(self, node, reward):\n",
        "        # Propagate the reward up the tree, updating visit counts and total rewards.\n",
        "        while node is not None:\n",
        "            node.visits += 1\n",
        "            node.total_reward += reward\n",
        "            node = node.parent\n",
        "\n",
        "    def run_simulation(self, root):\n",
        "        node = root\n",
        "        sim_env = self.create_env_from_state(node.state, node.score)\n",
        "\n",
        "        # TODO: Selection: Traverse the tree until reaching a non-fully expanded node.\n",
        "        while node.fully_expanded() and not sim_env.is_game_over():\n",
        "            node = self.select_child(node)\n",
        "            # Execute the action stored in the selected child.\n",
        "            sim_env.step(node.action)\n",
        "\n",
        "        # TODO: Expansion: if the node has untried actions, expand one.\n",
        "        if not sim_env.is_game_over():\n",
        "            if node.untried_actions:\n",
        "                action = random.choice(node.untried_actions)\n",
        "                node.untried_actions.remove(action)\n",
        "                # Apply the chosen action to the simulation environment.\n",
        "                sim_env.step(action)\n",
        "                # Create a new child node with the resulting state and score.\n",
        "                child_node = UCTNode(state=sim_env.board.copy(), score=sim_env.score, parent=node, action=action)\n",
        "                node.children[action] = child_node\n",
        "                node = child_node\n",
        "\n",
        "        # Rollout: Simulate a random game from the expanded node.\n",
        "        rollout_reward = self.rollout(sim_env, self.rollout_depth)\n",
        "        # Backpropagation: Update the tree with the rollout reward.\n",
        "        self.backpropagate(node, rollout_reward)\n",
        "\n",
        "    def best_action_distribution(self, root):\n",
        "        '''\n",
        "        Computes the visit count distribution for each action at the root node.\n",
        "        '''\n",
        "        total_visits = sum(child.visits for child in root.children.values())\n",
        "        distribution = np.zeros(4)\n",
        "        best_visits = -1\n",
        "        best_action = None\n",
        "        for action, child in root.children.items():\n",
        "            distribution[action] = child.visits / total_visits if total_visits > 0 else 0\n",
        "            if child.visits > best_visits:\n",
        "                best_visits = child.visits\n",
        "                best_action = action\n",
        "        return best_action, distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-1rLwJX4sw0"
      },
      "source": [
        "### Playing 2048 Using MCTS\n",
        "This section runs a 2048 game using Monte Carlo Tree Search (MCTS), selecting moves based on Upper Confidence Bound for Trees (UCT)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104347
        },
        "collapsed": true,
        "id": "T36nediRhFx9",
        "outputId": "0bf5241f-3362-4e83-bb96-d707a4959545"
      },
      "outputs": [],
      "source": [
        "env = Game2048Env()\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "# Instantiate the MCTS object with specified parameters\n",
        "# You can adjust these parameters to experiment with different strategies\n",
        "uct_mcts = UCTMCTS(env, iterations=50, exploration_constant=1.41, rollout_depth=10)\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    root = UCTNode(state, env.score)  # Initialize the root node for MCTS\n",
        "\n",
        "    # Run multiple simulations to construct and refine the search tree\n",
        "    for _ in range(uct_mcts.iterations):\n",
        "        uct_mcts.run_simulation(root)\n",
        "\n",
        "    # Select the best action based on the visit distribution of the root's children\n",
        "    best_action, visit_distribution = uct_mcts.best_action_distribution(root)\n",
        "    print(\"MCTS selected action:\", best_action, \"with visit distribution:\", visit_distribution)\n",
        "\n",
        "    state, reward, done, _ = env.step(best_action)\n",
        "    env.render(action=best_action)  # Display the updated game state\n",
        "\n",
        "\n",
        "print(\"Game over, final score:\", env.score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69eZPCYs40d0"
      },
      "source": [
        "## **2048 TD Learning with N-Tuple Approximation**\n",
        "\n",
        "### **What We Are Doing**\n",
        "- We use **Temporal Difference (TD) learning** to train an **N-Tuple function approximator**.\n",
        "- This approximator **estimates the value of a game state**, improving decision-making in 2048.\n",
        "- Instead of storing every state, we utilize **small tile patterns (n-tuples)** to approximate the game board.\n",
        "- Training involves **playing thousands of games** while updating weights based on rewards.\n",
        "\n",
        "### **How It Works**\n",
        "1. **Define N-Tuple Patterns**  \n",
        "   - These are small tile groups (e.g., rows, squares) used for function approximation.  \n",
        "   - **Rotations and flips** are applied to reduce redundancy and improve generalization.\n",
        "\n",
        "2. **TD Learning Process**\n",
        "   - The agent **plays games and learns** by updating value estimates.  \n",
        "   - An **Œµ-greedy policy** is used to balance exploration and exploitation.  \n",
        "   - **Weights are updated** based on the difference between **predicted** and **actual** rewards.\n",
        "\n",
        "3. **Train and Evaluate**\n",
        "   - We run **thousands of episodes** to refine the approximation.  \n",
        "   - Game performance is logged, tracking **final scores** and **how often the agent reaches 2048**.\n",
        "\n",
        "**Note:** There are no strict implementation rules for TD-learning‚Äîdesign your own pattern and implement TD-learning in any form (e.g., **TD(0), Multi-stage TD, or any TD variant**). However, remember to **briefly describe your approach** in the question later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-brvr3PWm4aT",
        "outputId": "21bee0a9-2090-40c3-c167-4aa2dc35c8fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 100/1000 | Avg Score: 1425.12 | Success Rate: 0.00\n",
            "Episode 200/1000 | Avg Score: 1329.92 | Success Rate: 0.00\n",
            "Episode 300/1000 | Avg Score: 1277.44 | Success Rate: 0.00\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-d509b9b89a37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;31m# Note: To achieve significantly better performance, you will likely need to train for over 100,000 episodes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;31m# However, to quickly verify that your implementation is working correctly, you can start by running it for 1,000 episodes before scaling up.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m \u001b[0mfinal_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapproximator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-d509b9b89a37>\u001b[0m in \u001b[0;36mtd_learning\u001b[0;34m(env, approximator, num_episodes, alpha, gamma, epsilon, lambda_param)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;31m# --- Take action in the real environment ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m             \u001b[0mincremental_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprevious_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mprevious_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ecd9840ae75a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mmoved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0mmoved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mmoved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ecd9840ae75a>\u001b[0m in \u001b[0;36mmove_right\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0moriginal_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mreversed_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mreversed_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mreversed_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mreversed_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-ecd9840ae75a>\u001b[0m in \u001b[0;36mcompress\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mnew_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mnew_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_row\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m    791\u001b[0m     \u001b[0;31m# Create array with final shape and original values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;31m# (padded area is undefined)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m     \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_area_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pad_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m     \u001b[0;31m# And prepare iteration over all dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;31m# (zipping may be more readable than using enumerate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/numpy/lib/arraypad.py\u001b[0m in \u001b[0;36m_pad_simple\u001b[0;34m(array, pad_width, fill_value)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mpadded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moriginal_area_slice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_area_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# -------------------------------\n",
        "# Transformation functions for symmetric sampling on a 4x4 board.\n",
        "\n",
        "def identity(pattern):\n",
        "    return pattern\n",
        "\n",
        "def rot90(pattern):\n",
        "    # Rotate 90 degrees clockwise:\n",
        "    # (row, col) -> (col, 3 - row)\n",
        "    return [(c, 3 - r) for (r, c) in pattern]\n",
        "\n",
        "def rot180(pattern):\n",
        "    # Rotate 180 degrees:\n",
        "    # (row, col) -> (3 - row, 3 - col)\n",
        "    return [(3 - r, 3 - c) for (r, c) in pattern]\n",
        "\n",
        "def rot270(pattern):\n",
        "    # Rotate 270 degrees clockwise:\n",
        "    # (row, col) -> (3 - col, r)\n",
        "    return [(3 - c, r) for (r, c) in pattern]\n",
        "\n",
        "def reflect_horizontal(pattern):\n",
        "    # Reflect over vertical axis:\n",
        "    # (row, col) -> (row, 3 - col)\n",
        "    return [(r, 3 - c) for (r, c) in pattern]\n",
        "\n",
        "def reflect_vertical(pattern):\n",
        "    # Reflect over horizontal axis:\n",
        "    # (row, col) -> (3 - row, c)\n",
        "    return [(3 - r, c) for (r, c) in pattern]\n",
        "\n",
        "class NTupleApproximator:\n",
        "    def __init__(self, board_size, patterns, lambda_param):\n",
        "        \"\"\" Initializes the N-Tuple approximator. 'patterns' is a list of base tuple patterns (each a list of (row, col) tuples). \"\"\"\n",
        "        self.board_size = board_size\n",
        "        self.patterns = patterns\n",
        "        # Create one weight dictionary per base pattern (shared across its symmetric variants)\n",
        "        self.weights = [defaultdict(float) for _ in patterns]\n",
        "        # For TD-lambda, initialize eligibility traces with the same structure.\n",
        "        self.eligibilities = [defaultdict(float) for _ in patterns]\n",
        "        # Instead of a flat list of all symmetric variants, we group them per base pattern.\n",
        "        self.symmetry_groups = []\n",
        "        for pattern in self.patterns:\n",
        "            syms = self.generate_symmetries(pattern)\n",
        "            self.symmetry_groups.append(syms)\n",
        "        self.lambda_param = lambda_param\n",
        "\n",
        "    def generate_symmetries(self, pattern):\n",
        "        # Generate 8 symmetrical transformations of the given pattern.\n",
        "        \"\"\"\n",
        "        Generate the eight symmetric transformations of the input pattern.\n",
        "        These include the identity, rotations (90, 180, 270 degrees) and\n",
        "        the horizontal reflections of each.\n",
        "        \"\"\"\n",
        "        sym = []\n",
        "        for transform in [identity, rot90, rot180, rot270]:\n",
        "            p = transform(pattern)\n",
        "            sym.append(p)\n",
        "            sym.append(reflect_horizontal(p))\n",
        "        # Remove any duplicates (if any symmetry maps the pattern onto itself)\n",
        "        unique = []\n",
        "        for s in sym:\n",
        "            if s not in unique:\n",
        "                unique.append(s)\n",
        "        return unique\n",
        "\n",
        "\n",
        "    def tile_to_index(self, tile):\n",
        "        \"\"\"\n",
        "        Converts tile values to an index for the lookup table.\n",
        "        \"\"\"\n",
        "        if tile == 0:\n",
        "            return 0\n",
        "        else:\n",
        "            return int(math.log(tile, 2))\n",
        "\n",
        "    def get_feature(self, board, coords):\n",
        "        # Extract tile values from the board based on the given coordinates and convert them into a feature tuple.\n",
        "        return tuple(self.tile_to_index(board[r, c]) for (r, c) in coords)\n",
        "\n",
        "    def value(self, board):\n",
        "        # Estimate the board value: sum the evaluations from all patterns.\n",
        "        total_value = 0.0\n",
        "        # Iterate over each base pattern's group.\n",
        "        for group_idx, group in enumerate(self.symmetry_groups):\n",
        "            # For each symmetric variant in the group:\n",
        "            for sym in group:\n",
        "                feature = self.get_feature(board, sym)\n",
        "                total_value += self.weights[group_idx][feature]\n",
        "        return total_value\n",
        "\n",
        "    def update(self, board, delta, alpha, lambda_param):\n",
        "        # Update weights based on the TD error.\n",
        "        for group_idx, group in enumerate(self.symmetry_groups):\n",
        "            num_sym = len(group)\n",
        "        for sym in group:\n",
        "            feature = self.get_feature(board, sym)\n",
        "            # Accumulate the eligibility trace for the active feature.\n",
        "            self.eligibilities[group_idx][feature] += 1.0\n",
        "            # Update the weight for this feature.\n",
        "            self.weights[group_idx][feature] += (alpha / num_sym) * delta * self.eligibilities[group_idx][feature]\n",
        "            # Decay the eligibility trace.\n",
        "            self.eligibilities[group_idx][feature] *= lambda_param\n",
        "\n",
        "\n",
        "def td_learning(env, approximator, num_episodes=50000, alpha=0.01, gamma=0.99, epsilon=0.1, lambda_param=0.9):\n",
        "    \"\"\"\n",
        "    Trains the 2048 agent using TD-Lambda Learning.\n",
        "\n",
        "    Args:\n",
        "        env: The 2048 game environment.\n",
        "        approximator: NTupleApproximator instance.\n",
        "        num_episodes: Number of training episodes.\n",
        "        alpha: Learning rate.\n",
        "        gamma: Discount factor.\n",
        "        epsilon: Epsilon-greedy exploration rate.\n",
        "        lambda_param: TD-lambda parameter.\n",
        "    \"\"\"\n",
        "    final_scores = []\n",
        "    success_flags = []\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        # Clear eligibility traces at the beginning of each episode.\n",
        "        for elig in approximator.eligibilities:\n",
        "            elig.clear()\n",
        "\n",
        "        previous_score = 0\n",
        "        done = False\n",
        "        max_tile = np.max(state)\n",
        "\n",
        "        while not done:\n",
        "            legal_moves = [a for a in range(4) if env.is_move_legal(a)]\n",
        "            if not legal_moves:\n",
        "                break\n",
        "\n",
        "            # --- Action Selection (Œµ-greedy) ---\n",
        "            if random.random() < epsilon:\n",
        "                # Exploration: choose a random legal move.\n",
        "                action = random.choice(legal_moves)\n",
        "            else:\n",
        "                # Exploitation: choose the move with the highest estimated value.\n",
        "                best_value = -float('inf')\n",
        "                best_action = None\n",
        "                # Evaluate each legal move by simulating the step.\n",
        "                for a in legal_moves:\n",
        "                    env_copy = copy.deepcopy(env)\n",
        "                    # Note: We use previous_score from the main env for consistency.\n",
        "                    sim_state, sim_score, sim_done, _ = env_copy.step(a)\n",
        "                    reward = sim_score - previous_score\n",
        "                    # Estimate value using immediate reward and discounted next state value.\n",
        "                    value_est = reward + gamma * approximator.value(sim_state)\n",
        "                    if value_est > best_value:\n",
        "                        best_value = value_est\n",
        "                        best_action = a\n",
        "                action = best_action\n",
        "\n",
        "            # --- Take action in the real environment ---\n",
        "            next_state, new_score, done, _ = env.step(action)\n",
        "            incremental_reward = new_score - previous_score\n",
        "            previous_score = new_score\n",
        "            max_tile = max(max_tile, np.max(next_state))\n",
        "\n",
        "            # --- TD Update ---\n",
        "            v_current = approximator.value(state)\n",
        "            # If episode terminates, we consider the value of the terminal state as 0.\n",
        "            v_next = 0 if done else approximator.value(next_state)\n",
        "            delta = incremental_reward + gamma * v_next - v_current\n",
        "            approximator.update(state, delta, alpha, lambda_param)\n",
        "\n",
        "            # Optionally, you could store trajectory information here.\n",
        "            state = next_state\n",
        "\n",
        "        final_scores.append(env.score)\n",
        "        success_flags.append(1 if max_tile >= 2048 else 0)\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_score = np.mean(final_scores[-100:])\n",
        "            success_rate = np.sum(success_flags[-100:]) / 100\n",
        "            print(f\"Episode {episode+1}/{num_episodes} | Avg Score: {avg_score:.2f} | Success Rate: {success_rate:.2f}\")\n",
        "\n",
        "    return final_scores\n",
        "\n",
        "# Example 5x6-tuple network patterns for a 4x4 board.\n",
        "# Each pattern is a list of six (row, col) coordinates.\n",
        "# These patterns are chosen to cover different parts of the board.\n",
        "patterns = [\n",
        "    # Pattern 1: top row and left part of second row\n",
        "    [(0, 0), (0, 1), (0, 2), (0, 3), (1, 0), (1, 1)],\n",
        "    # Pattern 2: second row and left part of third row\n",
        "    [(1, 0), (1, 1), (1, 2), (1, 3), (2, 0), (2, 1)],\n",
        "    # Pattern 3: thrid row and left part of fourth row\n",
        "    [(2, 0), (2, 1), (2, 2), (2, 3), (3, 0), (3, 1)],\n",
        "    # Pattern 4\n",
        "    [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)],\n",
        "    # Pattern 5\n",
        "    [(1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
        "]\n",
        "\n",
        "approximator = NTupleApproximator(board_size=4, patterns=patterns, lambda_param=0.9)\n",
        "\n",
        "env = Game2048Env()\n",
        "\n",
        "# Run TD-Learning training\n",
        "# Note: To achieve significantly better performance, you will likely need to train for over 100,000 episodes.\n",
        "# However, to quickly verify that your implementation is working correctly, you can start by running it for 1,000 episodes before scaling up.\n",
        "final_scores = td_learning(env, approximator, num_episodes=100000, alpha=0.1, gamma=0.99, lambda_param=0.9)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eczy63SGkP-p"
      },
      "source": [
        "**Question**: You are expected to provide a detailed explanation of your n-tuple design and TD-learning implementation.\n",
        "Additionally, you should discuss specific details such as:\n",
        "\n",
        "1Ô∏è‚É£ n-Tuple Design\n",
        "\n",
        "2Ô∏è‚É£ TD-Learning Implementation\n",
        "Is the board evaluated before or after adding a random tile?\n",
        "Is future board evaluation based on a single state or multiple states?\n",
        "Any other important details in your implementation?\n",
        "\n",
        "3Ô∏è‚É£ Training Results\n",
        "Provide a graph showing training steps vs. score progression.\n",
        "***Note: Ensure the explanation matches the implementation, or points will be deducted.***\n",
        "\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeUw9mAe596F"
      },
      "source": [
        "### Playing 2048 Using TD-Learned N-Tuple Approximation\n",
        "In this section, we will play 2048 using the N-Tuple function approximator trained with TD learning. Instead of random play or MCTS, the agent will now evaluate board states using learned value estimates to make more informed decisions.\n",
        "\n",
        "By using the trained approximator, the agent selects actions based on state value predictions, improving decision-making compared to random moves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpM5WLF67biN"
      },
      "outputs": [],
      "source": [
        "import copy  # Used for deep copying the environment\n",
        "import random\n",
        "\n",
        "# Initialize the game environment\n",
        "state = env.reset()\n",
        "env.render()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "\n",
        "    legal_moves = [a for a in range(4) if env.is_move_legal(a)]\n",
        "    if not legal_moves:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "    # TODO: Use your N-Tuple approximator to play 2048\n",
        "\n",
        "\n",
        "    action = best_action  # Choose the best action based on evaluation\n",
        "    state, reward, done, _ = env.step(action)  # Apply the selected action\n",
        "    env.render(action=action)  # Display the updated game state\n",
        "\n",
        "# Print final game results\n",
        "print(\"Game over, final score:\", env.score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTGF3mEe78u2"
      },
      "source": [
        "## **Integrating TD-Learned N-Tuple Approximation into MCTS**\n",
        "\n",
        "### **What We Are Doing?**\n",
        "- We are improving **Monte Carlo Tree Search (MCTS)** by integrating our **trained N-Tuple approximator**.\n",
        "- Instead of relying only on **random rollouts**, we now **estimate state values using TD-learning**.\n",
        "- This helps **MCTS make more accurate decisions**, especially in deeper searches.\n",
        "\n",
        "### **Key Modifications**\n",
        "1. **Using TD-Learned N-Tuple Approximation for Leaf Evaluation**\n",
        "   - Instead of running **random rollouts**, we **evaluate leaf nodes** using the **trained approximator**.\n",
        "   - This allows MCTS to **assess states more intelligently**.\n",
        "\n",
        "2. **Modified Rollout Phase**\n",
        "   - Instead of pure random rollouts, we:\n",
        "     - Play a **few random moves** (to allow some exploration).\n",
        "     - **Use the approximator to evaluate the final state**.\n",
        "   - This results in **faster and more accurate rollouts**.\n",
        "\n",
        "3. **Tree Search Steps Remain the Same**\n",
        "   - **Selection** ‚Äì Traverse the search tree using **UCT**.\n",
        "   - **Expansion** ‚Äì Add new nodes to explore untried actions.\n",
        "   - **Backpropagation** ‚Äì Update rewards and visit counts.\n",
        "\n",
        "### **NOTE: Do We Still Need Rollouts?**\n",
        "- Since **N-Tuple learning already estimates state values**, we **might not need random rollouts**.\n",
        "- However, **a few rollout moves** can help **reduce bias** and **improve exploration**.\n",
        "- This balance can be **tuned as a hyperparameter**.\n",
        "\n",
        "This approach **combines learning-based evaluation with tree search**, making MCTS much more efficient for 2048! üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bhjbJHHt38KX"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# Note: This MCTS implementation is almost identical to the previous one,\n",
        "# except for the rollout phase, which now incorporates the approximator.\n",
        "\n",
        "# Node for TD-MCTS using the TD-trained value approximator\n",
        "class TD_MCTS_Node:\n",
        "    def __init__(self, state, score, parent=None, action=None):\n",
        "        \"\"\"\n",
        "        state: current board state (numpy array)\n",
        "        score: cumulative score at this node\n",
        "        parent: parent node (None for root)\n",
        "        action: action taken from parent to reach this node\n",
        "        \"\"\"\n",
        "        self.state = state\n",
        "        self.score = score\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.total_reward = 0.0\n",
        "        # List of untried actions based on the current state's legal moves\n",
        "        self.untried_actions = [a for a in range(4) if env.is_move_legal(a)]\n",
        "\n",
        "    def fully_expanded(self):\n",
        "        # A node is fully expanded if no legal actions remain untried.\n",
        "        return len(self.untried_actions) == 0\n",
        "\n",
        "\n",
        "# TD-MCTS class utilizing a trained approximator for leaf evaluation\n",
        "class TD_MCTS:\n",
        "    def __init__(self, env, approximator, iterations=500, exploration_constant=1.41, rollout_depth=10, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.approximator = approximator\n",
        "        self.iterations = iterations\n",
        "        self.c = exploration_constant\n",
        "        self.rollout_depth = rollout_depth\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def create_env_from_state(self, state, score):\n",
        "        # Create a deep copy of the environment with the given state and score.\n",
        "        new_env = copy.deepcopy(self.env)\n",
        "        new_env.board = state.copy()\n",
        "        new_env.score = score\n",
        "        return new_env\n",
        "\n",
        "    def select_child(self, node):\n",
        "        # TODO: Use the UCT formula: Q + c * sqrt(log(parent.visits)/child.visits) to select the best child.\n",
        "\n",
        "\n",
        "    def rollout(self, sim_env, depth):\n",
        "        # TODO: Perform a random rollout until reaching the maximum depth or a terminal state.\n",
        "        # TODO: Use the approximator to evaluate the final state.\n",
        "\n",
        "\n",
        "    def backpropagate(self, node, reward):\n",
        "        # TODO: Propagate the obtained reward back up the tree.\n",
        "\n",
        "\n",
        "    def run_simulation(self, root):\n",
        "        node = root\n",
        "        sim_env = self.create_env_from_state(node.state, node.score)\n",
        "\n",
        "        # TODO: Selection: Traverse the tree until reaching an unexpanded node.\n",
        "\n",
        "\n",
        "        # TODO: Expansion: If the node is not terminal, expand an untried action.\n",
        "\n",
        "\n",
        "        # Rollout: Simulate a random game from the expanded node.\n",
        "        rollout_reward = self.rollout(sim_env, self.rollout_depth)\n",
        "        # Backpropagate the obtained reward.\n",
        "        self.backpropagate(node, rollout_reward)\n",
        "\n",
        "    def best_action_distribution(self, root):\n",
        "        # Compute the normalized visit count distribution for each child of the root.\n",
        "        total_visits = sum(child.visits for child in root.children.values())\n",
        "        distribution = np.zeros(4)\n",
        "        best_visits = -1\n",
        "        best_action = None\n",
        "        for action, child in root.children.items():\n",
        "            distribution[action] = child.visits / total_visits if total_visits > 0 else 0\n",
        "            if child.visits > best_visits:\n",
        "                best_visits = child.visits\n",
        "                best_action = action\n",
        "        return best_action, distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a_FgM87pJkX"
      },
      "source": [
        "**Question**: Clearly explain how you integrate the value approximator into MCTS to enhance decision-making.\n",
        "\n",
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxh3fbJJ8wDt"
      },
      "source": [
        "### **Playing 2048 Using TD-MCTS with Trained N-Tuple Table**\n",
        "\n",
        "- We are now playing **2048 using TD-MCTS**, which **combines tree search with our trained N-Tuple approximator**.\n",
        "- The **MCTS tree search** helps explore possible actions, while **TD-learned state values** guide decision-making.\n",
        "- This method **improves over pure MCTS** by providing **more accurate rollout estimates**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cedh4sP86ohp"
      },
      "outputs": [],
      "source": [
        "td_mcts = TD_MCTS(env, approximator, iterations=50, exploration_constant=1.41, rollout_depth=10, gamma=0.99)\n",
        "\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    # Create the root node from the current state\n",
        "    root = TD_MCTS_Node(state, env.score)\n",
        "\n",
        "    # Run multiple simulations to build the MCTS tree\n",
        "    for _ in range(td_mcts.iterations):\n",
        "        td_mcts.run_simulation(root)\n",
        "\n",
        "    # Select the best action (based on highest visit count)\n",
        "    best_act, _ = td_mcts.best_action_distribution(root)\n",
        "    print(\"TD-MCTS selected action:\", best_act)\n",
        "\n",
        "    # Execute the selected action and update the state\n",
        "    state, reward, done, _ = env.step(best_act)\n",
        "    env.render(action=best_act)\n",
        "\n",
        "print(\"Game over, final score:\", env.score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URuUldZKIa6j"
      },
      "source": [
        "# **Question 2: Self-Play Reinforcement Learning with PUCT-MCTS for 2048**  \n",
        "\n",
        "## **What We Are Doing**  \n",
        "In this question, you will implement a **self-play reinforcement learning process** similar to **AlphaZero**, but without neural networks. Instead, we will use **Monte Carlo Tree Search (MCTS)** to train a **policy approximator**, then integrate it into **PUCT-MCTS** for improved decision-making in 2048.  \n",
        "\n",
        "AlphaZero demonstrated that **combining self-play with MCTS and policy-value learning** leads to **superhuman performance** in games like Go and Chess. We will apply the same idea to **2048**, but using:  \n",
        "- **A policy approximator** (table-based, learned from MCTS visit counts).  \n",
        "- **A value approximator** (TD-learning with n-tuple features).  \n",
        "- **PUCT (Predictor + UCT) MCTS** to efficiently balance **exploration and exploitation**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Key Steps**  \n",
        "\n",
        "### **1. Train a Policy Approximator Using MCTS Self-Play**  \n",
        "- Run **MCTS simulations** and record the **visit count distribution** for each state.  \n",
        "- Use these distributions to **train a policy approximator** that predicts MCTS move probabilities.  \n",
        "\n",
        "### **2. Implement PUCT-MCTS (MCTS with Policy Guidance)**  \n",
        "- Modify **MCTS selection** to use the **PUCT formula**, combining:  \n",
        "  - **Value estimates** from TD-learning.  \n",
        "  - **Action priors** from the trained policy approximator.  \n",
        "\n",
        "$PUCT(s, a) = Q(s, a) + c_{puct} \\cdot P(s, a) \\cdot \\frac{\\sqrt{N(s)}}{1 + N(s, a)}$\n",
        "\n",
        "\n",
        "- This allows the **search tree to focus on promising moves earlier**, improving efficiency.  \n",
        "\n",
        "### **3. Further Improve Policy & Value Approximators with Self-Play**  \n",
        "- Continue **self-play training**, updating:  \n",
        "  - The **policy approximator** using **PUCT visit counts**.  \n",
        "  - The **value approximator** using **MCTS Q-values**.  \n",
        "- Over time, the learned policy can guide search more efficiently, reducing reliance on full MCTS simulations,\n",
        "while the value approximator enhances search by providing stronger state evaluations.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why This Works (Inspired by AlphaZero)**\n",
        "- **MCTS alone is computationally expensive**‚Äîadding a learned policy makes it much **faster and more efficient**.  \n",
        "- **Self-play allows the agent to improve over time**, adapting to new strategies.  \n",
        "- **Combining policy + value functions with PUCT makes MCTS significantly stronger**.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2rSE8Z_9hLe"
      },
      "source": [
        "## **Training a Policy Approximator Using MCTS (Inspired by AlphaZero)**\n",
        "\n",
        "### **What We Are Doing**\n",
        "- We use **TD-MCTS** to generate training data for a **policy approximator**.\n",
        "- The policy approximator **learns action probabilities** from **MCTS visit distributions**.\n",
        "- After training, the policy will be **used later in PUCT-MCTS**.\n",
        "\n",
        "### **Key Steps**\n",
        "1. **Run TD-MCTS on multiple games** to collect **state-action visit distributions**.\n",
        "2. **Train the policy approximator** by updating it toward MCTS action distributions.\n",
        "3. **Use the policy later** to improve **MCTS efficiency** (PUCT).\n",
        "\n",
        "### **Why This Works**\n",
        "- **MCTS alone** explores actions based on visit counts, which is **computationally expensive**.\n",
        "- **A trained policy** helps guide MCTS **more efficiently**.\n",
        "- Later, **we integrate this into PUCT-MCTS for better exploration**.\n",
        "\n",
        "\n",
        "üéØ **How to Earn 15 Points?**\n",
        "\n",
        "1Ô∏è‚É£ Correct Implementation of Policy Approximator and Training with MCTS\n",
        "- Successfully implement a **policy approximator** and train it using **MCTS**.(6 points)\n",
        "  - üî• **Your policy must achieve better performance than a random agent to be considered successfully trained.**\n",
        "- Discussion of the policy approximator\n",
        "\n",
        "2Ô∏è‚É£ Implementation of MCTS with PUCT Formula + Explanation (5 points)\n",
        "- Implement **Monte Carlo Tree Search (MCTS)** with the **PUCT formula**.\n",
        "- **Note:** Points may be deducted for:\n",
        "  - Errors in the PUCT formula.\n",
        "  - Incorrect function implementations.\n",
        "\n",
        "3Ô∏è‚É£ Implementation of Policy & Value Approximators with Self-Play (4 points)\n",
        "- Implement **self-play training** with both **policy & value approximators**.\n",
        "- The improved approximators should produce **more accurate value estimations**, leading to **better performance than previous versions**.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNi0p1aQjr3w"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# TODO: Define the action transformation functions (i.e., rot90_action, rot180_action, etc.)\n",
        "# Note: You have already defined transformation functions for patterns before.\n",
        "\n",
        "\n",
        "# Note: PolicyApproximator is similar to the value approximator but differs in key aspects.\n",
        "class PolicyApproximator:\n",
        "    def __init__(self, board_size, patterns):\n",
        "        \"\"\"\n",
        "        Initializes the N-Tuple approximator.\n",
        "        Hint: you can adjust these if you want.\n",
        "        \"\"\"\n",
        "        self.board_size = board_size\n",
        "        self.patterns = patterns\n",
        "        self.actions = [0, 1, 2, 3]\n",
        "        # Weight structure: [pattern_idx][feature_key][action]\n",
        "        self.weights = [defaultdict(lambda: defaultdict(float)) for _ in range(len(patterns))]\n",
        "        # Generate the 8 symmetrical transformations for each pattern and store their types.\n",
        "        self.symmetry_patterns = []\n",
        "        self.symmetry_types = []  # Store the type of symmetry transformation (rotation or reflection)\n",
        "        for pattern in self.patterns:\n",
        "            syms, types = self.generate_symmetries(pattern)\n",
        "            self.symmetry_patterns.extend(syms)\n",
        "            self.symmetry_types.extend(types)\n",
        "\n",
        "        # TODO: Define corresponding action transformation functions for each symmetry.\n",
        "\n",
        "\n",
        "    def generate_symmetries(self, pattern):\n",
        "        # TODO: Generate 8 symmetrical transformations of the given pattern.\n",
        "\n",
        "\n",
        "    def tile_to_index(self, tile):\n",
        "        return 0 if tile == 0 else int(math.log(tile, 2))\n",
        "\n",
        "    def get_feature(self, board, coords):\n",
        "        # TODO: Extract tile values from the board based on the given coordinates and convert them into a feature tuple.\n",
        "\n",
        "\n",
        "    def predict(self, board):\n",
        "        # TODO: Predict the policy (probability distribution over actions) given the board state.\n",
        "\n",
        "\n",
        "\n",
        "    def update(self, board, target_distribution, alpha=0.1):\n",
        "        # TODO: Update policy based on the target distribution.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGimbtmm7LKM"
      },
      "outputs": [],
      "source": [
        "def self_play_training_policy_with_td_mcts(env, td_mcts, policy_approximator, num_episodes=50):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Create the root node for the TD-MCTS tree\n",
        "            root = TD_MCTS_Node(state, env.score)\n",
        "\n",
        "            # Run multiple simulations to build the MCTS search tree\n",
        "            for _ in range(td_mcts.iterations):\n",
        "                td_mcts.run_simulation(root)\n",
        "\n",
        "            best_action, target_distribution = td_mcts.best_action_distribution(root)\n",
        "\n",
        "            # TODO: Update the NTuple Policy Approximator using the MCTS action distribution\n",
        "            # Here, we use the MCTS result directly as the label to update the policy\n",
        "\n",
        "\n",
        "\n",
        "            # Execute the selected action in the real environment\n",
        "            state, reward, done, _ = env.step(best_action)\n",
        "\n",
        "        print(f\"Episode {episode+1}/{num_episodes} finished, final score: {env.score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEsTDX6K7FuU"
      },
      "outputs": [],
      "source": [
        "env = Game2048Env()\n",
        "\n",
        "# TODO: Define your own pattern\n",
        "patterns = []\n",
        "\n",
        "#approximator = NTupleApproximator(board_size=4, patterns=patterns)\n",
        "\n",
        "policy_approximator = PolicyApproximator(board_size=4, patterns=patterns)\n",
        "td_mcts = TD_MCTS(env, approximator, iterations=50, exploration_constant=1.41, rollout_depth=10, gamma=0.99)\n",
        "\n",
        "self_play_training_policy_with_td_mcts(env, td_mcts, policy_approximator, num_episodes=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibRMhtq-J4qs"
      },
      "source": [
        "### **Policy Evaluation: Comparing Policy-Based and Random Agents**\n",
        "\n",
        "This cell **evaluates the performance of the learned policy approximator** by running multiple games and comparing its average score to a random agent.\n",
        "\n",
        "- The **policy-based agent** selects actions using the trained policy approximator, without any MCTS search.\n",
        "- The **random agent** selects actions uniformly at random as a baseline for comparison.\n",
        "- Each agent plays **10 games**, and their **average score and standard deviation** are recorded.\n",
        "\n",
        "By comparing the results, we can assess whether the policy has successfully learned a meaningful strategy.  \n",
        "- If the **policy consistently outperforms the random agent**, it indicates that training has been effective.  \n",
        "- If the scores are similar, the policy may not have learned useful patterns and may require further training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MtVf6-rJHWJ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_policy(env, policy_approximator, num_games=10):\n",
        "    scores = []\n",
        "    env.render()\n",
        "    for _ in range(num_games):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            # TODO: play with your policy approximator\n",
        "\n",
        "            state, reward, done, _ = env.step(best_action)\n",
        "            env.render(action=best_action)\n",
        "        scores.append(env.score)\n",
        "\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "def evaluate_random(env, num_games=10):\n",
        "    scores = []\n",
        "    env.render()\n",
        "    for _ in range(num_games):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            legal_moves = [a for a in range(4) if env.is_move_legal(a)]\n",
        "            if not legal_moves:\n",
        "                break\n",
        "            action = random.choice(legal_moves)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            env.render(action=action)\n",
        "        scores.append(env.score)\n",
        "    return np.mean(scores), np.std(scores)\n",
        "\n",
        "\n",
        "num_games = 10\n",
        "policy_mean, policy_std = evaluate_policy(env, policy_approximator, num_games)\n",
        "random_mean, random_std = evaluate_random(env, num_games)\n",
        "\n",
        "\n",
        "print(f\"Policy-based Agent - Avg Score: {policy_mean:.2f}, Std Dev: {policy_std:.2f}\")\n",
        "print(f\"Random Agent - Avg Score: {random_mean:.2f}, Std Dev: {random_std:.2f}\")\n",
        "\n",
        "# Visualization: Compare Policy vs. Random Agent\n",
        "labels = ['Policy-based Agent', 'Random Agent']\n",
        "means = [policy_mean, random_mean]\n",
        "std_devs = [policy_std, random_std]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, means, yerr=std_devs, capsize=10, color=['blue', 'red'])\n",
        "plt.ylabel('Average Score')\n",
        "plt.title('Policy vs. Random Agent Performance')\n",
        "plt.ylim(0, max(means) + max(std_devs) * 1.2)\n",
        "\n",
        "\n",
        "for i, v in enumerate(means):\n",
        "    plt.text(i, v + max(std_devs) * 0.1, f\"{v:.2f}\", ha='center', fontsize=12)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWLf-ZxSdYNc"
      },
      "source": [
        "**Question:**\n",
        "\n",
        "- Provide a graph comparing your policy agent and random agent performance.  \n",
        "\n",
        "- What is your tuple design?\n",
        "\n",
        "- After implementing the value and policy approximators, what is the main difference between these two?\n",
        "For example, what aspects of training require more attention for the policy approximator compared to the value approximator?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X05mxnce_CYk"
      },
      "source": [
        "## **PUCT-MCTS: Combining MCTS with Policy and Value Approximators**\n",
        "### **What We Are Doing**\n",
        "- We **integrate a trained policy approximator into MCTS** using the **PUCT formula**.\n",
        "- This **improves exploration efficiency** by guiding MCTS with **learned action priors**.\n",
        "- Instead of selecting moves **only by visit counts**, MCTS now considers **both value (Q) and policy priors (P)**.\n",
        "\n",
        "### **Key Modifications**\n",
        "1. **Selection uses the PUCT formula:**\n",
        "   $Q(s, a) + c_{puct} \\cdot P(s, a) \\cdot \\frac{\\sqrt{N(s)}}{1 + N(s, a)}$\n",
        "   - $Q(s, a)$: Average action value.\n",
        "   - $P(s, a)$: Prior probability from the **trained policy approximator**.\n",
        "   - $ N(s) $, $N(s, a)$ : Visit counts.\n",
        "   - $c_{puct}$ : Controls exploration.\n",
        "\n",
        "2. **Expansion phase now sets priors using the policy approximator.**\n",
        "3. **Rollout phase still uses the value approximator but is now optional.**\n",
        "4. **MCTS learns from past searches, improving decision-making over time.**\n",
        "\n",
        "### **Why This Works**\n",
        "- **Pure MCTS** spends too much time **exploring all moves equally**.\n",
        "- **PUCT guides search** using **pre-learned probabilities**, reducing unnecessary exploration.\n",
        "- This approach is **used in AlphaZero**, enabling **faster and stronger AI decision-making**.\n",
        "\n",
        "\n",
        "üöÄ **This method makes MCTS much more efficient and effective!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lcLe8tLQr_SZ"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class PUCTNode:\n",
        "    def __init__(self, state, score, parent=None, action=None, prior=0.0):\n",
        "        self.state = state\n",
        "        self.score = score\n",
        "        self.parent = parent\n",
        "        self.action = action\n",
        "        self.prior = prior\n",
        "        self.children = {}\n",
        "        self.visits = 0\n",
        "        self.total_reward = 0.0\n",
        "        self.untried_actions = [a for a in range(4) if env.is_move_legal(a)]\n",
        "\n",
        "    def fully_expanded(self):\n",
        "        return len(self.untried_actions) == 0\n",
        "\n",
        "\n",
        "class MCTS_PUCT:\n",
        "    def __init__(self, env, value_approximator, policy_approximator, iterations=500, c_puct=1.41, rollout_depth=10, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.value_approximator = value_approximator\n",
        "        self.policy_approximator = policy_approximator\n",
        "        self.iterations = iterations\n",
        "        self.c_puct = c_puct\n",
        "        self.rollout_depth = rollout_depth\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def create_env_from_state(self, state, score):\n",
        "        \"\"\"Creates a deep copy of the environment to simulate a given state.\"\"\"\n",
        "        new_env = copy.deepcopy(self.env)\n",
        "        new_env.board = state.copy()\n",
        "        new_env.score = score\n",
        "        return new_env\n",
        "\n",
        "    def select_child(self, node):\n",
        "        # TODO: Select the best child using the PUCT formula:\n",
        "        # PUCT(s,a) = Q(s,a) + c_puct * P(s,a) * sqrt(N(s)) / (1 + N(s,a))\n",
        "        # where Q(s,a) = child.total_reward / child.visits.\n",
        "\n",
        "\n",
        "    def rollout(self, sim_env, depth):\n",
        "        # TODO: Perform a random rollout until reaching the maximum depth or a terminal state.\n",
        "        # TODO: Use the approximator to evaluate the final state.\n",
        "        # Note: It's not necessary to perform rollouts if the value approximator is accurate.\n",
        "        value_est = self.value_approximator.value(sim_env.board)\n",
        "        return value_est\n",
        "\n",
        "    def backpropagate(self, node, reward):\n",
        "        # TODO: Propagate the reward up the tree, updating visit counts and total rewards.\n",
        "\n",
        "    def run_simulation(self, root):\n",
        "        node = root\n",
        "        sim_env = self.create_env_from_state(node.state, node.score)\n",
        "\n",
        "        # TODO: Selection phase: traverse the tree until reaching an expandable node.\n",
        "\n",
        "\n",
        "        # TODO: Expansion phase: if the node is not terminal, expand one untried action.\n",
        "\n",
        "\n",
        "        # Rollout phase: simulate random moves from the expanded node.\n",
        "        rollout_reward = self.rollout(sim_env, self.rollout_depth)\n",
        "        # Backpropagation phase: update the tree with the obtained reward.\n",
        "        self.backpropagate(node, rollout_reward)\n",
        "\n",
        "    def best_action_distribution(self, root):\n",
        "        total_visits = sum(child.visits for child in root.children.values())\n",
        "        distribution = np.zeros(4)\n",
        "        best_visits = -1\n",
        "        best_action = None\n",
        "        for action, child in root.children.items():\n",
        "            distribution[action] = child.visits / total_visits if total_visits > 0 else 0\n",
        "            if child.visits > best_visits:\n",
        "                best_visits = child.visits\n",
        "                best_action = action\n",
        "        return best_action, distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT5AYCE0gtOa"
      },
      "source": [
        "**Question:**  \n",
        "Implementation details of your **PUCT-MCTS**.  \n",
        "Since there are many differences between **PUCT-MCTS** and **UCT-MCTS** (policy, value), please provide a detailed explanation of your implementation.  \n",
        "For example, describe the differences in **selection, expansion, or other relevant aspects**.\n",
        "- ***Note: Ensure that your explanation matches the actual implementation, or points will be deducted.***  \n",
        "\n",
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA6mnQX9ALbt"
      },
      "source": [
        "### **Playing 2048 Using PUCT-MCTS with TD-Learned Approximators**\n",
        "- We play **2048 using PUCT-MCTS**, which combines **Monte Carlo Tree Search** with:\n",
        "- **A value approximator** trained via **TD-learning** (N-Tuple function).\n",
        "- **A policy approximator** trained via **self-play with MCTS**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tHZn3KS74ow"
      },
      "outputs": [],
      "source": [
        "env = Game2048Env()\n",
        "\n",
        "# Reset the environment and render the initial state\n",
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "mcts_puct = MCTS_PUCT(\n",
        "    env,\n",
        "    value_approximator=approximator,\n",
        "    policy_approximator=policy_approximator,\n",
        "    iterations=50,\n",
        "    c_puct=1.41,\n",
        "    rollout_depth=10,\n",
        "    gamma=0.99\n",
        ")\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    root = PUCTNode(state, env.score)\n",
        "    for _ in range(mcts_puct.iterations):\n",
        "        mcts_puct.run_simulation(root)\n",
        "\n",
        "    best_act, visit_distribution = mcts_puct.best_action_distribution(root)\n",
        "    print(\"PUCT selected action:\", best_act, \"with visit distribution:\", visit_distribution)\n",
        "\n",
        "    state, reward, done, _ = env.step(best_act)\n",
        "    env.render(action=best_act)\n",
        "\n",
        "print(\"Game over, final score:\", env.score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElchFNaaDx8W"
      },
      "source": [
        "## **Self-Play Training for PUCT-MCTS (Like AlphaZero, But Without Neural Networks!)**\n",
        "### **What We Are Doing**\n",
        "- We are implementing a **self-play reinforcement learning framework** similar to **AlphaZero**.\n",
        "- Instead of using **deep neural networks**, we use:\n",
        "  - **A policy approximator (table-based)**\n",
        "  - **A value approximator (N-Tuple function)**\n",
        "  - **Monte Carlo Tree Search with PUCT (MCTS-PUCT)**\n",
        "- This allows **efficient training** without requiring **GPU-heavy deep learning models**.\n",
        "\n",
        "### **Why This Is Like AlphaZero**\n",
        "1. **Self-play** is used to **train both policy & value approximators**.\n",
        "2. **MCTS guides exploration**, and the **policy learns from MCTS visit counts**.\n",
        "3. **The value function is updated** based on MCTS rollouts.\n",
        "4. **No pre-training is required**‚Äîthe agent **improves purely through self-play**.  \n",
        "   **(Note: Although pre-training is not required, we use a pre-trained value table to accelerate learning.)**\n",
        "\n",
        "### **Why This Works Without Neural Networks**\n",
        "- **N-Tuple approximators** efficiently estimate state values.\n",
        "- **Policy tables** generalize well in smaller action spaces (like 2048).\n",
        "- **PUCT-MCTS improves search efficiency**, making deep networks unnecessary.\n",
        "\n",
        "üöÄ **This is a lightweight, efficient version of AlphaZero, optimized for 2048!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOH3peRip2lT"
      },
      "outputs": [],
      "source": [
        "def self_play_training_policy_value(env, mcts_puct, policy_approximator, value_approximator, num_episodes=50, value_lr=0.01):\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Create the root node for the MCTS-PUCT tree\n",
        "            root = PUCTNode(state, env.score)\n",
        "\n",
        "            # Run multiple simulations to build the MCTS search tree\n",
        "            for _ in range(mcts_puct.iterations):\n",
        "                mcts_puct.run_simulation(root)\n",
        "\n",
        "            # TODO: Update the NTuple Policy Approximator using the MCTS action distribution\n",
        "\n",
        "\n",
        "            # TODO: Calculate the TD error for the value approximator and update your approximator\n",
        "\n",
        "            # Execute the selected action in the real environment\n",
        "            state, reward, done, _ = env.step(best_action)\n",
        "\n",
        "        print(f\"Episode {episode + 1}/{num_episodes} finished, final score: {env.score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mraHor9Y8Myj"
      },
      "outputs": [],
      "source": [
        "# Instantiate the PUCT MCTS object using the pre-trained NTuple-based approximators.\n",
        "\n",
        "mcts_puct = MCTS_PUCT(\n",
        "    env,\n",
        "    value_approximator=approximator,\n",
        "    policy_approximator=policy_approximator,\n",
        "    iterations=50,\n",
        "    c_puct=1.41,\n",
        "    rollout_depth=10,\n",
        "    gamma=0.99\n",
        ")\n",
        "\n",
        "# Run the self-play training loop to further update both the policy and value approximators.\n",
        "self_play_training_policy_value(env, mcts_puct, policy_approximator, approximator,\n",
        "                                num_episodes=10, value_lr=0.01)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgvlXE12i_rC"
      },
      "source": [
        "**Question:**\n",
        "- Provide a **graph showing training steps vs. score progression** to illustrate learning performance.  \n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFTO0YOOFRZl"
      },
      "source": [
        "### **Playing 2048 Using PUCT-MCTS with Trained Approximators**\n",
        "- Now that we have **trained the policy and value approximators** through self-play, we **test them in actual gameplay**.\n",
        "- The agent will play **2048 using PUCT-MCTS**, making **smarter, more efficient decisions**.\n",
        "- This is the **final step**, where we evaluate how well our training has improved the agent.\n",
        "\n",
        "\n",
        "üöÄ **Now, let's see how well our trained agent plays 2048!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD0PYVB19QTd"
      },
      "outputs": [],
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "\n",
        "mcts_puct = MCTS_PUCT(\n",
        "    env,\n",
        "    value_approximator=approximator,\n",
        "    policy_approximator=policy_approximator,\n",
        "    iterations=50,\n",
        "    c_puct=1.41,\n",
        "    rollout_depth=10,\n",
        "    gamma=0.99\n",
        ")\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    root = PUCTNode(state, env.score)\n",
        "    for _ in range(mcts_puct.iterations):\n",
        "        mcts_puct.run_simulation(root)\n",
        "\n",
        "    best_act, visit_distribution = mcts_puct.best_action_distribution(root)\n",
        "    print(\"PUCT selected action:\", best_act, \"with visit distribution:\", visit_distribution)\n",
        "\n",
        "    state, reward, done, _ = env.step(best_act)\n",
        "    env.render(action=best_act)\n",
        "\n",
        "\n",
        "# Print final results\n",
        "max_tile = np.max(state)  # Get the highest tile achieved\n",
        "print(f\"Game over! Final score: {env.score}, Highest tile: {max_tile}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M5mPKud8HkH"
      },
      "source": [
        "üí° Tip: Make the most of these two questions (Q1 & Q2) to understand key techniques‚Äîthey can be directly applied to Q3 to maximize your score! Keep pushing forward, and aim for the best results! üí™üî•"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
